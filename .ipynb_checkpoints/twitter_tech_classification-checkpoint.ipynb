{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from nlp_helper import read_glove_vecs, convert_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载训练数据\n",
    "pos_datas = np.load(\"../../data/twitter/tech_not/pos_cleaned.npy\")\n",
    "neg_datas = np.load(\"../../data/twitter/tech_not/neg_cleaned.npy\")\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(pos_datas)\n",
    "np.random.shuffle(neg_datas)\n",
    "\n",
    "pos_datas_train, pos_datas_test = pos_datas[:700],pos_datas[700:800]\n",
    "neg_datas_train_10, neg_datas_test_10 = neg_datas[:7000],neg_datas[7000:8000]\n",
    "neg_datas_train_5, neg_datas_test_5 = neg_datas[13600:17100],neg_datas[17100:20000]\n",
    "neg_datas_train_1_2 = neg_datas[20000:20350]\n",
    "neg_datas_train_2, neg_datas_test_2 = neg_datas[12000:13400],neg_datas[13400:13600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_to_ix, _, w_to_vec_map  = read_glove_vecs(\"../../data/embedding/glove.twitter.27B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train1_2 = np.concatenate((pos_datas_train,neg_datas_train_2), axis = 0)\n",
    "\n",
    "# np.random.shuffle(train1_2)\n",
    "# X_train_1_2 = train1_2[:,1]\n",
    "# Y_train_1_2 = np.array(list(map(int, train1_2[:,0])))\n",
    "# train1_5 = np.concatenate((pos_datas_train,neg_datas_train_5), axis = 0)\n",
    "\n",
    "# np.random.shuffle(train1_5)\n",
    "# X_train_1_5 = train1_5[:,1]\n",
    "# Y_train_1_5 = np.array(list(map(int, train1_5[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_datas_50augs = np.tile(pos_datas, (50,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auged = np.concatenate((pos_datas_50augs,neg_datas), axis = 0)\n",
    "\n",
    "np.random.shuffle(train_auged )\n",
    "\n",
    "X_train, X_test = train_auged[:,1][0:80000], train_auged[:,1][80000:]\n",
    "Y_train, Y_test = np.array(list(map(int, train_auged[:,0])))[0:80000], np.array(list(map(int, train_auged[:,0])))[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = nlp(X[i].lower())\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        \n",
    "        if len(sentence_words)> max_len:\n",
    "            continue\n",
    "        # Loop over the words of sentence_words\n",
    "        else:\n",
    "            j = 0\n",
    "            for w in sentence_words:\n",
    "\n",
    "                try:\n",
    "                    X_indices[i, j] = word_to_index[w.text]\n",
    "                    j += 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "        print(\"indexing 已完成:{}/{}\".format(i+1,m),end='\\r')\n",
    "                \n",
    "\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_vec_map[\"cucumber\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        if word_to_vec_map[word].shape[0]!=50 :\n",
    "            print(\"word:\", word)\n",
    "            continue\n",
    "        emb_matrix[index] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Emojify_V2\n",
    "\n",
    "def twitter_tech_classify_V1(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype ='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(2)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation(activation='softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs =sentence_indices ,outputs=X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxLen = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 80, 50)            59675700  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 80, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 80, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 59,899,190\n",
      "Trainable params: 223,490\n",
      "Non-trainable params: 59,675,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = twitter_tech_classify_V1((maxLen,), w_to_vec_map, w_to_ix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成:79999/80000\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, w_to_ix, maxLen)\n",
    "X_train_indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"x_train_indices.npy\", X_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_oh = convert_to_one_hot(Y_train, C = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80000/80000 [==============================] - ETA: 25s - loss: 0.2232 - acc: 0.92 - ETA: 23s - loss: 0.1886 - acc: 0.94 - ETA: 22s - loss: 0.1719 - acc: 0.94 - ETA: 21s - loss: 0.1618 - acc: 0.94 - ETA: 21s - loss: 0.1632 - acc: 0.94 - ETA: 21s - loss: 0.1574 - acc: 0.94 - ETA: 20s - loss: 0.1503 - acc: 0.95 - ETA: 20s - loss: 0.1511 - acc: 0.95 - ETA: 20s - loss: 0.1477 - acc: 0.95 - ETA: 20s - loss: 0.1485 - acc: 0.95 - ETA: 19s - loss: 0.1511 - acc: 0.95 - ETA: 19s - loss: 0.1485 - acc: 0.95 - ETA: 18s - loss: 0.1465 - acc: 0.95 - ETA: 18s - loss: 0.1467 - acc: 0.95 - ETA: 18s - loss: 0.1465 - acc: 0.95 - ETA: 17s - loss: 0.1457 - acc: 0.95 - ETA: 17s - loss: 0.1452 - acc: 0.95 - ETA: 17s - loss: 0.1446 - acc: 0.95 - ETA: 16s - loss: 0.1421 - acc: 0.95 - ETA: 16s - loss: 0.1396 - acc: 0.95 - ETA: 16s - loss: 0.1388 - acc: 0.95 - ETA: 15s - loss: 0.1395 - acc: 0.95 - ETA: 15s - loss: 0.1392 - acc: 0.95 - ETA: 15s - loss: 0.1382 - acc: 0.95 - ETA: 15s - loss: 0.1366 - acc: 0.95 - ETA: 14s - loss: 0.1376 - acc: 0.95 - ETA: 14s - loss: 0.1356 - acc: 0.95 - ETA: 14s - loss: 0.1351 - acc: 0.95 - ETA: 13s - loss: 0.1349 - acc: 0.95 - ETA: 13s - loss: 0.1356 - acc: 0.95 - ETA: 13s - loss: 0.1357 - acc: 0.95 - ETA: 13s - loss: 0.1361 - acc: 0.95 - ETA: 12s - loss: 0.1353 - acc: 0.95 - ETA: 12s - loss: 0.1352 - acc: 0.95 - ETA: 12s - loss: 0.1334 - acc: 0.96 - ETA: 11s - loss: 0.1323 - acc: 0.96 - ETA: 11s - loss: 0.1322 - acc: 0.96 - ETA: 11s - loss: 0.1324 - acc: 0.96 - ETA: 11s - loss: 0.1315 - acc: 0.96 - ETA: 10s - loss: 0.1307 - acc: 0.96 - ETA: 10s - loss: 0.1302 - acc: 0.96 - ETA: 10s - loss: 0.1300 - acc: 0.96 - ETA: 9s - loss: 0.1295 - acc: 0.9615 - ETA: 9s - loss: 0.1290 - acc: 0.961 - ETA: 9s - loss: 0.1281 - acc: 0.962 - ETA: 9s - loss: 0.1269 - acc: 0.962 - ETA: 8s - loss: 0.1264 - acc: 0.962 - ETA: 8s - loss: 0.1260 - acc: 0.963 - ETA: 8s - loss: 0.1256 - acc: 0.963 - ETA: 7s - loss: 0.1242 - acc: 0.963 - ETA: 7s - loss: 0.1235 - acc: 0.964 - ETA: 7s - loss: 0.1227 - acc: 0.964 - ETA: 7s - loss: 0.1218 - acc: 0.964 - ETA: 6s - loss: 0.1212 - acc: 0.965 - ETA: 6s - loss: 0.1218 - acc: 0.964 - ETA: 6s - loss: 0.1220 - acc: 0.964 - ETA: 5s - loss: 0.1214 - acc: 0.965 - ETA: 5s - loss: 0.1214 - acc: 0.965 - ETA: 5s - loss: 0.1216 - acc: 0.965 - ETA: 5s - loss: 0.1213 - acc: 0.965 - ETA: 4s - loss: 0.1211 - acc: 0.965 - ETA: 4s - loss: 0.1214 - acc: 0.965 - ETA: 4s - loss: 0.1211 - acc: 0.965 - ETA: 4s - loss: 0.1212 - acc: 0.965 - ETA: 3s - loss: 0.1208 - acc: 0.965 - ETA: 3s - loss: 0.1203 - acc: 0.965 - ETA: 3s - loss: 0.1201 - acc: 0.965 - ETA: 2s - loss: 0.1199 - acc: 0.965 - ETA: 2s - loss: 0.1195 - acc: 0.966 - ETA: 2s - loss: 0.1194 - acc: 0.966 - ETA: 2s - loss: 0.1190 - acc: 0.966 - ETA: 1s - loss: 0.1187 - acc: 0.966 - ETA: 1s - loss: 0.1182 - acc: 0.966 - ETA: 1s - loss: 0.1180 - acc: 0.966 - ETA: 0s - loss: 0.1177 - acc: 0.966 - ETA: 0s - loss: 0.1176 - acc: 0.966 - ETA: 0s - loss: 0.1173 - acc: 0.967 - ETA: 0s - loss: 0.1172 - acc: 0.967 - 22s 279us/step - loss: 0.1171 - acc: 0.9671\n",
      "Epoch 2/10\n",
      "80000/80000 [==============================] - ETA: 25s - loss: 0.0863 - acc: 0.97 - ETA: 24s - loss: 0.0939 - acc: 0.97 - ETA: 22s - loss: 0.0991 - acc: 0.97 - ETA: 21s - loss: 0.1031 - acc: 0.97 - ETA: 21s - loss: 0.0979 - acc: 0.97 - ETA: 20s - loss: 0.0957 - acc: 0.97 - ETA: 20s - loss: 0.0939 - acc: 0.97 - ETA: 20s - loss: 0.0962 - acc: 0.97 - ETA: 19s - loss: 0.0988 - acc: 0.97 - ETA: 19s - loss: 0.0990 - acc: 0.97 - ETA: 19s - loss: 0.0989 - acc: 0.97 - ETA: 18s - loss: 0.1005 - acc: 0.97 - ETA: 18s - loss: 0.0995 - acc: 0.97 - ETA: 18s - loss: 0.0997 - acc: 0.97 - ETA: 17s - loss: 0.1029 - acc: 0.97 - ETA: 17s - loss: 0.1028 - acc: 0.97 - ETA: 17s - loss: 0.1026 - acc: 0.97 - ETA: 16s - loss: 0.1037 - acc: 0.97 - ETA: 16s - loss: 0.1033 - acc: 0.97 - ETA: 16s - loss: 0.1029 - acc: 0.97 - ETA: 16s - loss: 0.1019 - acc: 0.97 - ETA: 15s - loss: 0.1006 - acc: 0.97 - ETA: 15s - loss: 0.1000 - acc: 0.97 - ETA: 15s - loss: 0.0995 - acc: 0.97 - ETA: 15s - loss: 0.0987 - acc: 0.97 - ETA: 14s - loss: 0.0982 - acc: 0.97 - ETA: 14s - loss: 0.0991 - acc: 0.97 - ETA: 14s - loss: 0.0983 - acc: 0.97 - ETA: 13s - loss: 0.0981 - acc: 0.97 - ETA: 13s - loss: 0.0976 - acc: 0.97 - ETA: 13s - loss: 0.0963 - acc: 0.97 - ETA: 13s - loss: 0.0960 - acc: 0.97 - ETA: 12s - loss: 0.0964 - acc: 0.97 - ETA: 12s - loss: 0.0953 - acc: 0.97 - ETA: 12s - loss: 0.0953 - acc: 0.97 - ETA: 11s - loss: 0.0960 - acc: 0.97 - ETA: 11s - loss: 0.0961 - acc: 0.97 - ETA: 11s - loss: 0.0956 - acc: 0.97 - ETA: 11s - loss: 0.0963 - acc: 0.97 - ETA: 10s - loss: 0.0955 - acc: 0.97 - ETA: 10s - loss: 0.0952 - acc: 0.97 - ETA: 10s - loss: 0.0945 - acc: 0.97 - ETA: 9s - loss: 0.0944 - acc: 0.9757 - ETA: 9s - loss: 0.0944 - acc: 0.975 - ETA: 9s - loss: 0.0941 - acc: 0.975 - ETA: 9s - loss: 0.0941 - acc: 0.975 - ETA: 8s - loss: 0.0939 - acc: 0.975 - ETA: 8s - loss: 0.0932 - acc: 0.976 - ETA: 8s - loss: 0.0931 - acc: 0.975 - ETA: 7s - loss: 0.0931 - acc: 0.976 - ETA: 7s - loss: 0.0928 - acc: 0.976 - ETA: 7s - loss: 0.0924 - acc: 0.976 - ETA: 7s - loss: 0.0928 - acc: 0.976 - ETA: 6s - loss: 0.0926 - acc: 0.976 - ETA: 6s - loss: 0.0926 - acc: 0.976 - ETA: 6s - loss: 0.0932 - acc: 0.976 - ETA: 5s - loss: 0.0935 - acc: 0.976 - ETA: 5s - loss: 0.0933 - acc: 0.976 - ETA: 5s - loss: 0.0933 - acc: 0.976 - ETA: 5s - loss: 0.0933 - acc: 0.976 - ETA: 4s - loss: 0.0938 - acc: 0.975 - ETA: 4s - loss: 0.0936 - acc: 0.975 - ETA: 4s - loss: 0.0936 - acc: 0.975 - ETA: 4s - loss: 0.0932 - acc: 0.976 - ETA: 3s - loss: 0.0933 - acc: 0.976 - ETA: 3s - loss: 0.0933 - acc: 0.976 - ETA: 3s - loss: 0.0936 - acc: 0.976 - ETA: 2s - loss: 0.0931 - acc: 0.976 - ETA: 2s - loss: 0.0931 - acc: 0.976 - ETA: 2s - loss: 0.0933 - acc: 0.976 - ETA: 2s - loss: 0.0933 - acc: 0.976 - ETA: 1s - loss: 0.0929 - acc: 0.976 - ETA: 1s - loss: 0.0930 - acc: 0.976 - ETA: 1s - loss: 0.0927 - acc: 0.976 - ETA: 0s - loss: 0.0925 - acc: 0.976 - ETA: 0s - loss: 0.0920 - acc: 0.976 - ETA: 0s - loss: 0.0915 - acc: 0.976 - ETA: 0s - loss: 0.0916 - acc: 0.976 - 22s 280us/step - loss: 0.0917 - acc: 0.9768\n",
      "Epoch 3/10\n",
      "80000/80000 [==============================] - ETA: 21s - loss: 0.0876 - acc: 0.97 - ETA: 21s - loss: 0.0958 - acc: 0.97 - ETA: 20s - loss: 0.0978 - acc: 0.97 - ETA: 20s - loss: 0.0934 - acc: 0.97 - ETA: 20s - loss: 0.0928 - acc: 0.97 - ETA: 20s - loss: 0.0969 - acc: 0.97 - ETA: 20s - loss: 0.0958 - acc: 0.97 - ETA: 20s - loss: 0.0939 - acc: 0.97 - ETA: 19s - loss: 0.0913 - acc: 0.97 - ETA: 19s - loss: 0.0915 - acc: 0.97 - ETA: 19s - loss: 0.0911 - acc: 0.97 - ETA: 19s - loss: 0.0936 - acc: 0.97 - ETA: 18s - loss: 0.0920 - acc: 0.97 - ETA: 18s - loss: 0.0897 - acc: 0.97 - ETA: 18s - loss: 0.0882 - acc: 0.97 - ETA: 17s - loss: 0.0895 - acc: 0.97 - ETA: 17s - loss: 0.0891 - acc: 0.97 - ETA: 17s - loss: 0.0889 - acc: 0.97 - ETA: 16s - loss: 0.0895 - acc: 0.97 - ETA: 16s - loss: 0.0891 - acc: 0.97 - ETA: 16s - loss: 0.0895 - acc: 0.97 - ETA: 15s - loss: 0.0901 - acc: 0.97 - ETA: 15s - loss: 0.0905 - acc: 0.97 - ETA: 15s - loss: 0.0902 - acc: 0.97 - ETA: 15s - loss: 0.0898 - acc: 0.97 - ETA: 14s - loss: 0.0897 - acc: 0.97 - ETA: 14s - loss: 0.0902 - acc: 0.97 - ETA: 14s - loss: 0.0893 - acc: 0.97 - ETA: 13s - loss: 0.0892 - acc: 0.97 - ETA: 13s - loss: 0.0902 - acc: 0.97 - ETA: 13s - loss: 0.0907 - acc: 0.97 - ETA: 13s - loss: 0.0911 - acc: 0.97 - ETA: 12s - loss: 0.0903 - acc: 0.97 - ETA: 12s - loss: 0.0892 - acc: 0.97 - ETA: 12s - loss: 0.0894 - acc: 0.97 - ETA: 11s - loss: 0.0886 - acc: 0.97 - ETA: 11s - loss: 0.0882 - acc: 0.97 - ETA: 11s - loss: 0.0881 - acc: 0.97 - ETA: 11s - loss: 0.0877 - acc: 0.97 - ETA: 10s - loss: 0.0875 - acc: 0.97 - ETA: 10s - loss: 0.0866 - acc: 0.97 - ETA: 10s - loss: 0.0859 - acc: 0.97 - ETA: 10s - loss: 0.0856 - acc: 0.97 - ETA: 9s - loss: 0.0851 - acc: 0.9785 - ETA: 9s - loss: 0.0854 - acc: 0.978 - ETA: 9s - loss: 0.0848 - acc: 0.978 - ETA: 8s - loss: 0.0844 - acc: 0.978 - ETA: 8s - loss: 0.0843 - acc: 0.978 - ETA: 8s - loss: 0.0843 - acc: 0.979 - ETA: 8s - loss: 0.0835 - acc: 0.979 - ETA: 7s - loss: 0.0836 - acc: 0.979 - ETA: 7s - loss: 0.0832 - acc: 0.979 - ETA: 7s - loss: 0.0829 - acc: 0.979 - ETA: 6s - loss: 0.0828 - acc: 0.979 - ETA: 6s - loss: 0.0828 - acc: 0.979 - ETA: 6s - loss: 0.0823 - acc: 0.979 - ETA: 6s - loss: 0.0819 - acc: 0.979 - ETA: 5s - loss: 0.0818 - acc: 0.979 - ETA: 5s - loss: 0.0819 - acc: 0.979 - ETA: 5s - loss: 0.0816 - acc: 0.979 - ETA: 4s - loss: 0.0815 - acc: 0.979 - ETA: 4s - loss: 0.0815 - acc: 0.979 - ETA: 4s - loss: 0.0815 - acc: 0.979 - ETA: 4s - loss: 0.0816 - acc: 0.979 - ETA: 3s - loss: 0.0816 - acc: 0.979 - ETA: 3s - loss: 0.0820 - acc: 0.979 - ETA: 3s - loss: 0.0817 - acc: 0.979 - ETA: 2s - loss: 0.0816 - acc: 0.979 - ETA: 2s - loss: 0.0814 - acc: 0.979 - ETA: 2s - loss: 0.0815 - acc: 0.979 - ETA: 2s - loss: 0.0815 - acc: 0.979 - ETA: 1s - loss: 0.0819 - acc: 0.979 - ETA: 1s - loss: 0.0821 - acc: 0.979 - ETA: 1s - loss: 0.0818 - acc: 0.979 - ETA: 0s - loss: 0.0820 - acc: 0.979 - ETA: 0s - loss: 0.0822 - acc: 0.979 - ETA: 0s - loss: 0.0822 - acc: 0.979 - ETA: 0s - loss: 0.0823 - acc: 0.979 - 23s 284us/step - loss: 0.0822 - acc: 0.9795\n",
      "Epoch 4/10\n",
      "80000/80000 [==============================] - ETA: 28s - loss: 0.0746 - acc: 0.98 - ETA: 25s - loss: 0.0769 - acc: 0.98 - ETA: 24s - loss: 0.0777 - acc: 0.97 - ETA: 22s - loss: 0.0761 - acc: 0.98 - ETA: 22s - loss: 0.0778 - acc: 0.98 - ETA: 21s - loss: 0.0779 - acc: 0.98 - ETA: 20s - loss: 0.0789 - acc: 0.98 - ETA: 20s - loss: 0.0758 - acc: 0.98 - ETA: 20s - loss: 0.0750 - acc: 0.98 - ETA: 19s - loss: 0.0777 - acc: 0.98 - ETA: 19s - loss: 0.0764 - acc: 0.98 - ETA: 18s - loss: 0.0754 - acc: 0.98 - ETA: 18s - loss: 0.0742 - acc: 0.98 - ETA: 18s - loss: 0.0745 - acc: 0.98 - ETA: 18s - loss: 0.0726 - acc: 0.98 - ETA: 17s - loss: 0.0734 - acc: 0.98 - ETA: 17s - loss: 0.0732 - acc: 0.98 - ETA: 17s - loss: 0.0735 - acc: 0.98 - ETA: 17s - loss: 0.0749 - acc: 0.98 - ETA: 16s - loss: 0.0744 - acc: 0.98 - ETA: 16s - loss: 0.0746 - acc: 0.98 - ETA: 16s - loss: 0.0742 - acc: 0.98 - ETA: 15s - loss: 0.0744 - acc: 0.98 - ETA: 15s - loss: 0.0755 - acc: 0.98 - ETA: 15s - loss: 0.0758 - acc: 0.98 - ETA: 14s - loss: 0.0778 - acc: 0.98 - ETA: 14s - loss: 0.0790 - acc: 0.98 - ETA: 14s - loss: 0.0787 - acc: 0.98 - ETA: 13s - loss: 0.0785 - acc: 0.98 - ETA: 13s - loss: 0.0793 - acc: 0.98 - ETA: 13s - loss: 0.0798 - acc: 0.98 - ETA: 13s - loss: 0.0809 - acc: 0.97 - ETA: 12s - loss: 0.0821 - acc: 0.97 - ETA: 12s - loss: 0.0833 - acc: 0.97 - ETA: 12s - loss: 0.0841 - acc: 0.97 - ETA: 12s - loss: 0.0850 - acc: 0.97 - ETA: 11s - loss: 0.0850 - acc: 0.97 - ETA: 11s - loss: 0.0848 - acc: 0.97 - ETA: 11s - loss: 0.0858 - acc: 0.97 - ETA: 10s - loss: 0.0854 - acc: 0.97 - ETA: 10s - loss: 0.0853 - acc: 0.97 - ETA: 10s - loss: 0.0850 - acc: 0.97 - ETA: 10s - loss: 0.0849 - acc: 0.97 - ETA: 9s - loss: 0.0843 - acc: 0.9784 - ETA: 9s - loss: 0.0847 - acc: 0.978 - ETA: 9s - loss: 0.0846 - acc: 0.978 - ETA: 8s - loss: 0.0846 - acc: 0.978 - ETA: 8s - loss: 0.0842 - acc: 0.978 - ETA: 8s - loss: 0.0845 - acc: 0.978 - ETA: 8s - loss: 0.0844 - acc: 0.978 - ETA: 7s - loss: 0.0845 - acc: 0.978 - ETA: 7s - loss: 0.0849 - acc: 0.978 - ETA: 7s - loss: 0.0853 - acc: 0.978 - ETA: 6s - loss: 0.0850 - acc: 0.978 - ETA: 6s - loss: 0.0859 - acc: 0.978 - ETA: 6s - loss: 0.0857 - acc: 0.978 - ETA: 6s - loss: 0.0855 - acc: 0.978 - ETA: 5s - loss: 0.0856 - acc: 0.978 - ETA: 5s - loss: 0.0858 - acc: 0.978 - ETA: 5s - loss: 0.0859 - acc: 0.978 - ETA: 4s - loss: 0.0858 - acc: 0.978 - ETA: 4s - loss: 0.0855 - acc: 0.978 - ETA: 4s - loss: 0.0859 - acc: 0.978 - ETA: 4s - loss: 0.0859 - acc: 0.978 - ETA: 3s - loss: 0.0856 - acc: 0.978 - ETA: 3s - loss: 0.0850 - acc: 0.978 - ETA: 3s - loss: 0.0847 - acc: 0.979 - ETA: 2s - loss: 0.0843 - acc: 0.979 - ETA: 2s - loss: 0.0842 - acc: 0.979 - ETA: 2s - loss: 0.0840 - acc: 0.979 - ETA: 2s - loss: 0.0837 - acc: 0.979 - ETA: 1s - loss: 0.0832 - acc: 0.979 - ETA: 1s - loss: 0.0828 - acc: 0.979 - ETA: 1s - loss: 0.0822 - acc: 0.979 - ETA: 0s - loss: 0.0821 - acc: 0.979 - ETA: 0s - loss: 0.0823 - acc: 0.980 - ETA: 0s - loss: 0.0822 - acc: 0.980 - ETA: 0s - loss: 0.0822 - acc: 0.980 - 22s 281us/step - loss: 0.0821 - acc: 0.9801\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - ETA: 21s - loss: 0.0711 - acc: 0.98 - ETA: 21s - loss: 0.0829 - acc: 0.98 - ETA: 21s - loss: 0.0848 - acc: 0.98 - ETA: 20s - loss: 0.0883 - acc: 0.97 - ETA: 20s - loss: 0.0930 - acc: 0.97 - ETA: 20s - loss: 0.0916 - acc: 0.97 - ETA: 19s - loss: 0.0917 - acc: 0.97 - ETA: 19s - loss: 0.0901 - acc: 0.97 - ETA: 19s - loss: 0.0913 - acc: 0.97 - ETA: 19s - loss: 0.0934 - acc: 0.97 - ETA: 19s - loss: 0.0947 - acc: 0.97 - ETA: 18s - loss: 0.0984 - acc: 0.97 - ETA: 18s - loss: 0.0973 - acc: 0.97 - ETA: 18s - loss: 0.0962 - acc: 0.97 - ETA: 17s - loss: 0.0942 - acc: 0.97 - ETA: 17s - loss: 0.0920 - acc: 0.97 - ETA: 17s - loss: 0.0906 - acc: 0.97 - ETA: 17s - loss: 0.0912 - acc: 0.97 - ETA: 16s - loss: 0.0912 - acc: 0.97 - ETA: 16s - loss: 0.0915 - acc: 0.97 - ETA: 16s - loss: 0.0907 - acc: 0.97 - ETA: 15s - loss: 0.0902 - acc: 0.97 - ETA: 15s - loss: 0.0901 - acc: 0.97 - ETA: 15s - loss: 0.0902 - acc: 0.97 - ETA: 15s - loss: 0.0896 - acc: 0.97 - ETA: 14s - loss: 0.0887 - acc: 0.97 - ETA: 14s - loss: 0.0885 - acc: 0.97 - ETA: 14s - loss: 0.0876 - acc: 0.97 - ETA: 14s - loss: 0.0869 - acc: 0.97 - ETA: 13s - loss: 0.0862 - acc: 0.97 - ETA: 13s - loss: 0.0857 - acc: 0.97 - ETA: 13s - loss: 0.0857 - acc: 0.97 - ETA: 12s - loss: 0.0855 - acc: 0.97 - ETA: 12s - loss: 0.0848 - acc: 0.97 - ETA: 12s - loss: 0.0844 - acc: 0.97 - ETA: 11s - loss: 0.0838 - acc: 0.97 - ETA: 11s - loss: 0.0827 - acc: 0.97 - ETA: 11s - loss: 0.0831 - acc: 0.97 - ETA: 11s - loss: 0.0831 - acc: 0.97 - ETA: 10s - loss: 0.0831 - acc: 0.97 - ETA: 10s - loss: 0.0831 - acc: 0.97 - ETA: 10s - loss: 0.0828 - acc: 0.98 - ETA: 9s - loss: 0.0825 - acc: 0.9802 - ETA: 9s - loss: 0.0820 - acc: 0.980 - ETA: 9s - loss: 0.0817 - acc: 0.980 - ETA: 9s - loss: 0.0807 - acc: 0.980 - ETA: 8s - loss: 0.0799 - acc: 0.981 - ETA: 8s - loss: 0.0797 - acc: 0.981 - ETA: 8s - loss: 0.0796 - acc: 0.981 - ETA: 8s - loss: 0.0793 - acc: 0.981 - ETA: 7s - loss: 0.0790 - acc: 0.981 - ETA: 7s - loss: 0.0788 - acc: 0.981 - ETA: 7s - loss: 0.0785 - acc: 0.981 - ETA: 6s - loss: 0.0784 - acc: 0.981 - ETA: 6s - loss: 0.0783 - acc: 0.981 - ETA: 6s - loss: 0.0782 - acc: 0.981 - ETA: 5s - loss: 0.0782 - acc: 0.981 - ETA: 5s - loss: 0.0780 - acc: 0.981 - ETA: 5s - loss: 0.0777 - acc: 0.981 - ETA: 5s - loss: 0.0774 - acc: 0.981 - ETA: 4s - loss: 0.0770 - acc: 0.982 - ETA: 4s - loss: 0.0766 - acc: 0.982 - ETA: 4s - loss: 0.0761 - acc: 0.982 - ETA: 4s - loss: 0.0761 - acc: 0.982 - ETA: 3s - loss: 0.0758 - acc: 0.982 - ETA: 3s - loss: 0.0755 - acc: 0.982 - ETA: 3s - loss: 0.0747 - acc: 0.982 - ETA: 2s - loss: 0.0742 - acc: 0.982 - ETA: 2s - loss: 0.0736 - acc: 0.983 - ETA: 2s - loss: 0.0735 - acc: 0.983 - ETA: 2s - loss: 0.0731 - acc: 0.983 - ETA: 1s - loss: 0.0726 - acc: 0.983 - ETA: 1s - loss: 0.0721 - acc: 0.983 - ETA: 1s - loss: 0.0727 - acc: 0.983 - ETA: 0s - loss: 0.0725 - acc: 0.983 - ETA: 0s - loss: 0.0721 - acc: 0.983 - ETA: 0s - loss: 0.0720 - acc: 0.983 - ETA: 0s - loss: 0.0718 - acc: 0.983 - 22s 279us/step - loss: 0.0719 - acc: 0.9836\n",
      "Epoch 6/10\n",
      "80000/80000 [==============================] - ETA: 22s - loss: 0.0866 - acc: 0.97 - ETA: 23s - loss: 0.0842 - acc: 0.98 - ETA: 23s - loss: 0.0757 - acc: 0.98 - ETA: 22s - loss: 0.0735 - acc: 0.98 - ETA: 21s - loss: 0.0771 - acc: 0.98 - ETA: 20s - loss: 0.0745 - acc: 0.98 - ETA: 20s - loss: 0.0760 - acc: 0.98 - ETA: 20s - loss: 0.0771 - acc: 0.98 - ETA: 19s - loss: 0.0774 - acc: 0.98 - ETA: 19s - loss: 0.0757 - acc: 0.98 - ETA: 19s - loss: 0.0757 - acc: 0.98 - ETA: 18s - loss: 0.0763 - acc: 0.98 - ETA: 18s - loss: 0.0766 - acc: 0.98 - ETA: 18s - loss: 0.0754 - acc: 0.98 - ETA: 17s - loss: 0.0737 - acc: 0.98 - ETA: 17s - loss: 0.0735 - acc: 0.98 - ETA: 17s - loss: 0.0739 - acc: 0.98 - ETA: 17s - loss: 0.0736 - acc: 0.98 - ETA: 16s - loss: 0.0724 - acc: 0.98 - ETA: 16s - loss: 0.0712 - acc: 0.98 - ETA: 16s - loss: 0.0700 - acc: 0.98 - ETA: 16s - loss: 0.0705 - acc: 0.98 - ETA: 15s - loss: 0.0705 - acc: 0.98 - ETA: 15s - loss: 0.0697 - acc: 0.98 - ETA: 15s - loss: 0.0704 - acc: 0.98 - ETA: 14s - loss: 0.0706 - acc: 0.98 - ETA: 14s - loss: 0.0710 - acc: 0.98 - ETA: 14s - loss: 0.0704 - acc: 0.98 - ETA: 14s - loss: 0.0704 - acc: 0.98 - ETA: 13s - loss: 0.0703 - acc: 0.98 - ETA: 13s - loss: 0.0705 - acc: 0.98 - ETA: 13s - loss: 0.0698 - acc: 0.98 - ETA: 12s - loss: 0.0689 - acc: 0.98 - ETA: 12s - loss: 0.0690 - acc: 0.98 - ETA: 12s - loss: 0.0688 - acc: 0.98 - ETA: 12s - loss: 0.0689 - acc: 0.98 - ETA: 11s - loss: 0.0693 - acc: 0.98 - ETA: 11s - loss: 0.0692 - acc: 0.98 - ETA: 11s - loss: 0.0694 - acc: 0.98 - ETA: 10s - loss: 0.0689 - acc: 0.98 - ETA: 10s - loss: 0.0679 - acc: 0.98 - ETA: 10s - loss: 0.0681 - acc: 0.98 - ETA: 10s - loss: 0.0675 - acc: 0.98 - ETA: 9s - loss: 0.0673 - acc: 0.9848 - ETA: 9s - loss: 0.0673 - acc: 0.984 - ETA: 9s - loss: 0.0671 - acc: 0.984 - ETA: 8s - loss: 0.0667 - acc: 0.985 - ETA: 8s - loss: 0.0666 - acc: 0.985 - ETA: 8s - loss: 0.0669 - acc: 0.985 - ETA: 7s - loss: 0.0665 - acc: 0.985 - ETA: 7s - loss: 0.0665 - acc: 0.985 - ETA: 7s - loss: 0.0668 - acc: 0.985 - ETA: 7s - loss: 0.0672 - acc: 0.985 - ETA: 6s - loss: 0.0670 - acc: 0.985 - ETA: 6s - loss: 0.0669 - acc: 0.985 - ETA: 6s - loss: 0.0671 - acc: 0.985 - ETA: 6s - loss: 0.0672 - acc: 0.985 - ETA: 5s - loss: 0.0672 - acc: 0.985 - ETA: 5s - loss: 0.0671 - acc: 0.985 - ETA: 5s - loss: 0.0669 - acc: 0.985 - ETA: 4s - loss: 0.0672 - acc: 0.985 - ETA: 4s - loss: 0.0670 - acc: 0.985 - ETA: 4s - loss: 0.0671 - acc: 0.985 - ETA: 4s - loss: 0.0671 - acc: 0.985 - ETA: 3s - loss: 0.0673 - acc: 0.985 - ETA: 3s - loss: 0.0671 - acc: 0.985 - ETA: 3s - loss: 0.0672 - acc: 0.985 - ETA: 2s - loss: 0.0672 - acc: 0.985 - ETA: 2s - loss: 0.0676 - acc: 0.985 - ETA: 2s - loss: 0.0675 - acc: 0.985 - ETA: 2s - loss: 0.0676 - acc: 0.985 - ETA: 1s - loss: 0.0680 - acc: 0.984 - ETA: 1s - loss: 0.0677 - acc: 0.985 - ETA: 1s - loss: 0.0683 - acc: 0.984 - ETA: 0s - loss: 0.0688 - acc: 0.984 - ETA: 0s - loss: 0.0691 - acc: 0.984 - ETA: 0s - loss: 0.0695 - acc: 0.984 - ETA: 0s - loss: 0.0702 - acc: 0.983 - 23s 282us/step - loss: 0.0702 - acc: 0.9838\n",
      "Epoch 7/10\n",
      "80000/80000 [==============================] - ETA: 21s - loss: 0.0876 - acc: 0.98 - ETA: 20s - loss: 0.0885 - acc: 0.98 - ETA: 20s - loss: 0.0836 - acc: 0.98 - ETA: 20s - loss: 0.0820 - acc: 0.97 - ETA: 20s - loss: 0.0843 - acc: 0.97 - ETA: 19s - loss: 0.0831 - acc: 0.98 - ETA: 19s - loss: 0.0785 - acc: 0.98 - ETA: 19s - loss: 0.0765 - acc: 0.98 - ETA: 19s - loss: 0.0796 - acc: 0.98 - ETA: 19s - loss: 0.0800 - acc: 0.98 - ETA: 19s - loss: 0.0813 - acc: 0.98 - ETA: 18s - loss: 0.0800 - acc: 0.98 - ETA: 18s - loss: 0.0803 - acc: 0.98 - ETA: 18s - loss: 0.0790 - acc: 0.98 - ETA: 17s - loss: 0.0772 - acc: 0.98 - ETA: 17s - loss: 0.0765 - acc: 0.98 - ETA: 17s - loss: 0.0759 - acc: 0.98 - ETA: 17s - loss: 0.0753 - acc: 0.98 - ETA: 16s - loss: 0.0753 - acc: 0.98 - ETA: 16s - loss: 0.0746 - acc: 0.98 - ETA: 16s - loss: 0.0738 - acc: 0.98 - ETA: 15s - loss: 0.0731 - acc: 0.98 - ETA: 15s - loss: 0.0723 - acc: 0.98 - ETA: 15s - loss: 0.0723 - acc: 0.98 - ETA: 14s - loss: 0.0719 - acc: 0.98 - ETA: 14s - loss: 0.0718 - acc: 0.98 - ETA: 14s - loss: 0.0714 - acc: 0.98 - ETA: 14s - loss: 0.0706 - acc: 0.98 - ETA: 14s - loss: 0.0702 - acc: 0.98 - ETA: 13s - loss: 0.0703 - acc: 0.98 - ETA: 13s - loss: 0.0699 - acc: 0.98 - ETA: 13s - loss: 0.0700 - acc: 0.98 - ETA: 12s - loss: 0.0698 - acc: 0.98 - ETA: 12s - loss: 0.0692 - acc: 0.98 - ETA: 12s - loss: 0.0686 - acc: 0.98 - ETA: 12s - loss: 0.0679 - acc: 0.98 - ETA: 11s - loss: 0.0670 - acc: 0.98 - ETA: 11s - loss: 0.0660 - acc: 0.98 - ETA: 11s - loss: 0.0658 - acc: 0.98 - ETA: 10s - loss: 0.0653 - acc: 0.98 - ETA: 10s - loss: 0.0650 - acc: 0.98 - ETA: 10s - loss: 0.0645 - acc: 0.98 - ETA: 10s - loss: 0.0649 - acc: 0.98 - ETA: 9s - loss: 0.0647 - acc: 0.9861 - ETA: 9s - loss: 0.0654 - acc: 0.985 - ETA: 9s - loss: 0.0654 - acc: 0.985 - ETA: 8s - loss: 0.0661 - acc: 0.985 - ETA: 8s - loss: 0.0658 - acc: 0.985 - ETA: 8s - loss: 0.0663 - acc: 0.985 - ETA: 8s - loss: 0.0665 - acc: 0.985 - ETA: 7s - loss: 0.0665 - acc: 0.985 - ETA: 7s - loss: 0.0666 - acc: 0.985 - ETA: 7s - loss: 0.0663 - acc: 0.985 - ETA: 6s - loss: 0.0665 - acc: 0.985 - ETA: 6s - loss: 0.0663 - acc: 0.985 - ETA: 6s - loss: 0.0661 - acc: 0.985 - ETA: 6s - loss: 0.0659 - acc: 0.985 - ETA: 5s - loss: 0.0655 - acc: 0.985 - ETA: 5s - loss: 0.0656 - acc: 0.985 - ETA: 5s - loss: 0.0652 - acc: 0.985 - ETA: 4s - loss: 0.0648 - acc: 0.985 - ETA: 4s - loss: 0.0646 - acc: 0.985 - ETA: 4s - loss: 0.0644 - acc: 0.985 - ETA: 4s - loss: 0.0642 - acc: 0.985 - ETA: 3s - loss: 0.0647 - acc: 0.985 - ETA: 3s - loss: 0.0645 - acc: 0.985 - ETA: 3s - loss: 0.0643 - acc: 0.985 - ETA: 2s - loss: 0.0644 - acc: 0.985 - ETA: 2s - loss: 0.0642 - acc: 0.986 - ETA: 2s - loss: 0.0640 - acc: 0.986 - ETA: 2s - loss: 0.0642 - acc: 0.986 - ETA: 1s - loss: 0.0639 - acc: 0.986 - ETA: 1s - loss: 0.0638 - acc: 0.986 - ETA: 1s - loss: 0.0638 - acc: 0.986 - ETA: 0s - loss: 0.0637 - acc: 0.986 - ETA: 0s - loss: 0.0637 - acc: 0.986 - ETA: 0s - loss: 0.0635 - acc: 0.986 - ETA: 0s - loss: 0.0641 - acc: 0.986 - 22s 280us/step - loss: 0.0640 - acc: 0.9862\n",
      "Epoch 8/10\n",
      "80000/80000 [==============================] - ETA: 20s - loss: 0.0478 - acc: 0.99 - ETA: 23s - loss: 0.0627 - acc: 0.98 - ETA: 23s - loss: 0.0611 - acc: 0.98 - ETA: 22s - loss: 0.0566 - acc: 0.98 - ETA: 22s - loss: 0.0584 - acc: 0.98 - ETA: 22s - loss: 0.0621 - acc: 0.98 - ETA: 21s - loss: 0.0648 - acc: 0.98 - ETA: 20s - loss: 0.0697 - acc: 0.98 - ETA: 20s - loss: 0.0693 - acc: 0.98 - ETA: 20s - loss: 0.0702 - acc: 0.98 - ETA: 19s - loss: 0.0709 - acc: 0.98 - ETA: 19s - loss: 0.0696 - acc: 0.98 - ETA: 19s - loss: 0.0686 - acc: 0.98 - ETA: 18s - loss: 0.0688 - acc: 0.98 - ETA: 18s - loss: 0.0677 - acc: 0.98 - ETA: 18s - loss: 0.0698 - acc: 0.98 - ETA: 17s - loss: 0.0685 - acc: 0.98 - ETA: 17s - loss: 0.0686 - acc: 0.98 - ETA: 17s - loss: 0.0680 - acc: 0.98 - ETA: 17s - loss: 0.0679 - acc: 0.98 - ETA: 16s - loss: 0.0674 - acc: 0.98 - ETA: 16s - loss: 0.0671 - acc: 0.98 - ETA: 16s - loss: 0.0655 - acc: 0.98 - ETA: 15s - loss: 0.0656 - acc: 0.98 - ETA: 15s - loss: 0.0652 - acc: 0.98 - ETA: 15s - loss: 0.0637 - acc: 0.98 - ETA: 14s - loss: 0.0625 - acc: 0.98 - ETA: 14s - loss: 0.0628 - acc: 0.98 - ETA: 14s - loss: 0.0626 - acc: 0.98 - ETA: 13s - loss: 0.0632 - acc: 0.98 - ETA: 13s - loss: 0.0631 - acc: 0.98 - ETA: 13s - loss: 0.0641 - acc: 0.98 - ETA: 13s - loss: 0.0637 - acc: 0.98 - ETA: 12s - loss: 0.0649 - acc: 0.98 - ETA: 12s - loss: 0.0654 - acc: 0.98 - ETA: 12s - loss: 0.0655 - acc: 0.98 - ETA: 11s - loss: 0.0652 - acc: 0.98 - ETA: 11s - loss: 0.0656 - acc: 0.98 - ETA: 11s - loss: 0.0667 - acc: 0.98 - ETA: 11s - loss: 0.0672 - acc: 0.98 - ETA: 10s - loss: 0.0674 - acc: 0.98 - ETA: 10s - loss: 0.0672 - acc: 0.98 - ETA: 10s - loss: 0.0671 - acc: 0.98 - ETA: 9s - loss: 0.0669 - acc: 0.9850 - ETA: 9s - loss: 0.0666 - acc: 0.985 - ETA: 9s - loss: 0.0668 - acc: 0.985 - ETA: 8s - loss: 0.0667 - acc: 0.985 - ETA: 8s - loss: 0.0663 - acc: 0.985 - ETA: 8s - loss: 0.0654 - acc: 0.985 - ETA: 8s - loss: 0.0658 - acc: 0.985 - ETA: 7s - loss: 0.0654 - acc: 0.985 - ETA: 7s - loss: 0.0652 - acc: 0.985 - ETA: 7s - loss: 0.0650 - acc: 0.985 - ETA: 6s - loss: 0.0651 - acc: 0.985 - ETA: 6s - loss: 0.0650 - acc: 0.985 - ETA: 6s - loss: 0.0647 - acc: 0.985 - ETA: 6s - loss: 0.0641 - acc: 0.986 - ETA: 5s - loss: 0.0637 - acc: 0.986 - ETA: 5s - loss: 0.0635 - acc: 0.986 - ETA: 5s - loss: 0.0635 - acc: 0.986 - ETA: 4s - loss: 0.0632 - acc: 0.986 - ETA: 4s - loss: 0.0628 - acc: 0.986 - ETA: 4s - loss: 0.0624 - acc: 0.986 - ETA: 4s - loss: 0.0622 - acc: 0.986 - ETA: 3s - loss: 0.0619 - acc: 0.986 - ETA: 3s - loss: 0.0615 - acc: 0.986 - ETA: 3s - loss: 0.0612 - acc: 0.986 - ETA: 2s - loss: 0.0612 - acc: 0.986 - ETA: 2s - loss: 0.0611 - acc: 0.986 - ETA: 2s - loss: 0.0608 - acc: 0.986 - ETA: 2s - loss: 0.0611 - acc: 0.986 - ETA: 1s - loss: 0.0609 - acc: 0.987 - ETA: 1s - loss: 0.0609 - acc: 0.987 - ETA: 1s - loss: 0.0606 - acc: 0.987 - ETA: 0s - loss: 0.0606 - acc: 0.987 - ETA: 0s - loss: 0.0604 - acc: 0.987 - ETA: 0s - loss: 0.0600 - acc: 0.987 - ETA: 0s - loss: 0.0603 - acc: 0.987 - 23s 281us/step - loss: 0.0603 - acc: 0.9871\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - ETA: 22s - loss: 0.0918 - acc: 0.97 - ETA: 21s - loss: 0.0766 - acc: 0.98 - ETA: 21s - loss: 0.0957 - acc: 0.97 - ETA: 21s - loss: 0.0908 - acc: 0.97 - ETA: 20s - loss: 0.0829 - acc: 0.97 - ETA: 20s - loss: 0.0834 - acc: 0.97 - ETA: 20s - loss: 0.0802 - acc: 0.97 - ETA: 19s - loss: 0.0852 - acc: 0.97 - ETA: 19s - loss: 0.0840 - acc: 0.97 - ETA: 19s - loss: 0.0817 - acc: 0.97 - ETA: 19s - loss: 0.0827 - acc: 0.97 - ETA: 19s - loss: 0.0848 - acc: 0.97 - ETA: 18s - loss: 0.0835 - acc: 0.97 - ETA: 18s - loss: 0.0828 - acc: 0.97 - ETA: 18s - loss: 0.0815 - acc: 0.97 - ETA: 17s - loss: 0.0829 - acc: 0.97 - ETA: 17s - loss: 0.0836 - acc: 0.97 - ETA: 17s - loss: 0.0828 - acc: 0.98 - ETA: 16s - loss: 0.0826 - acc: 0.98 - ETA: 16s - loss: 0.0825 - acc: 0.98 - ETA: 16s - loss: 0.0820 - acc: 0.98 - ETA: 15s - loss: 0.0811 - acc: 0.98 - ETA: 15s - loss: 0.0812 - acc: 0.98 - ETA: 15s - loss: 0.0807 - acc: 0.98 - ETA: 15s - loss: 0.0808 - acc: 0.98 - ETA: 14s - loss: 0.0806 - acc: 0.98 - ETA: 14s - loss: 0.0788 - acc: 0.98 - ETA: 14s - loss: 0.0782 - acc: 0.98 - ETA: 14s - loss: 0.0785 - acc: 0.98 - ETA: 13s - loss: 0.0780 - acc: 0.98 - ETA: 13s - loss: 0.0788 - acc: 0.98 - ETA: 13s - loss: 0.0785 - acc: 0.98 - ETA: 13s - loss: 0.0777 - acc: 0.98 - ETA: 12s - loss: 0.0767 - acc: 0.98 - ETA: 12s - loss: 0.0765 - acc: 0.98 - ETA: 12s - loss: 0.0764 - acc: 0.98 - ETA: 11s - loss: 0.0768 - acc: 0.98 - ETA: 11s - loss: 0.0764 - acc: 0.98 - ETA: 11s - loss: 0.0761 - acc: 0.98 - ETA: 11s - loss: 0.0758 - acc: 0.98 - ETA: 10s - loss: 0.0750 - acc: 0.98 - ETA: 10s - loss: 0.0742 - acc: 0.98 - ETA: 10s - loss: 0.0731 - acc: 0.98 - ETA: 9s - loss: 0.0724 - acc: 0.9836 - ETA: 9s - loss: 0.0722 - acc: 0.983 - ETA: 9s - loss: 0.0717 - acc: 0.983 - ETA: 9s - loss: 0.0717 - acc: 0.983 - ETA: 8s - loss: 0.0718 - acc: 0.983 - ETA: 8s - loss: 0.0712 - acc: 0.984 - ETA: 8s - loss: 0.0705 - acc: 0.984 - ETA: 7s - loss: 0.0697 - acc: 0.984 - ETA: 7s - loss: 0.0695 - acc: 0.984 - ETA: 7s - loss: 0.0687 - acc: 0.984 - ETA: 6s - loss: 0.0680 - acc: 0.985 - ETA: 6s - loss: 0.0675 - acc: 0.985 - ETA: 6s - loss: 0.0671 - acc: 0.985 - ETA: 6s - loss: 0.0669 - acc: 0.985 - ETA: 5s - loss: 0.0663 - acc: 0.985 - ETA: 5s - loss: 0.0660 - acc: 0.985 - ETA: 5s - loss: 0.0655 - acc: 0.985 - ETA: 4s - loss: 0.0651 - acc: 0.985 - ETA: 4s - loss: 0.0652 - acc: 0.985 - ETA: 4s - loss: 0.0650 - acc: 0.985 - ETA: 4s - loss: 0.0647 - acc: 0.985 - ETA: 3s - loss: 0.0644 - acc: 0.986 - ETA: 3s - loss: 0.0640 - acc: 0.986 - ETA: 3s - loss: 0.0638 - acc: 0.986 - ETA: 2s - loss: 0.0636 - acc: 0.986 - ETA: 2s - loss: 0.0636 - acc: 0.986 - ETA: 2s - loss: 0.0632 - acc: 0.986 - ETA: 2s - loss: 0.0632 - acc: 0.986 - ETA: 1s - loss: 0.0631 - acc: 0.986 - ETA: 1s - loss: 0.0631 - acc: 0.986 - ETA: 1s - loss: 0.0631 - acc: 0.986 - ETA: 0s - loss: 0.0633 - acc: 0.986 - ETA: 0s - loss: 0.0633 - acc: 0.986 - ETA: 0s - loss: 0.0634 - acc: 0.986 - ETA: 0s - loss: 0.0636 - acc: 0.986 - 23s 281us/step - loss: 0.0636 - acc: 0.9861\n",
      "Epoch 10/10\n",
      "80000/80000 [==============================] - ETA: 22s - loss: 0.0638 - acc: 0.98 - ETA: 23s - loss: 0.0579 - acc: 0.99 - ETA: 22s - loss: 0.0588 - acc: 0.98 - ETA: 21s - loss: 0.0631 - acc: 0.98 - ETA: 21s - loss: 0.0659 - acc: 0.98 - ETA: 21s - loss: 0.0669 - acc: 0.98 - ETA: 20s - loss: 0.0632 - acc: 0.98 - ETA: 20s - loss: 0.0631 - acc: 0.98 - ETA: 20s - loss: 0.0621 - acc: 0.98 - ETA: 19s - loss: 0.0582 - acc: 0.98 - ETA: 19s - loss: 0.0589 - acc: 0.98 - ETA: 18s - loss: 0.0589 - acc: 0.98 - ETA: 18s - loss: 0.0564 - acc: 0.98 - ETA: 18s - loss: 0.0559 - acc: 0.98 - ETA: 18s - loss: 0.0550 - acc: 0.98 - ETA: 17s - loss: 0.0553 - acc: 0.98 - ETA: 17s - loss: 0.0549 - acc: 0.98 - ETA: 17s - loss: 0.0555 - acc: 0.98 - ETA: 17s - loss: 0.0568 - acc: 0.98 - ETA: 17s - loss: 0.0572 - acc: 0.98 - ETA: 16s - loss: 0.0572 - acc: 0.98 - ETA: 16s - loss: 0.0567 - acc: 0.98 - ETA: 16s - loss: 0.0572 - acc: 0.98 - ETA: 15s - loss: 0.0565 - acc: 0.98 - ETA: 15s - loss: 0.0563 - acc: 0.98 - ETA: 15s - loss: 0.0561 - acc: 0.98 - ETA: 15s - loss: 0.0565 - acc: 0.98 - ETA: 15s - loss: 0.0567 - acc: 0.98 - ETA: 14s - loss: 0.0571 - acc: 0.98 - ETA: 14s - loss: 0.0579 - acc: 0.98 - ETA: 14s - loss: 0.0575 - acc: 0.98 - ETA: 13s - loss: 0.0575 - acc: 0.98 - ETA: 13s - loss: 0.0572 - acc: 0.98 - ETA: 13s - loss: 0.0573 - acc: 0.98 - ETA: 12s - loss: 0.0567 - acc: 0.98 - ETA: 12s - loss: 0.0570 - acc: 0.98 - ETA: 12s - loss: 0.0567 - acc: 0.98 - ETA: 12s - loss: 0.0570 - acc: 0.98 - ETA: 11s - loss: 0.0564 - acc: 0.98 - ETA: 11s - loss: 0.0557 - acc: 0.98 - ETA: 11s - loss: 0.0552 - acc: 0.98 - ETA: 10s - loss: 0.0547 - acc: 0.98 - ETA: 10s - loss: 0.0544 - acc: 0.98 - ETA: 10s - loss: 0.0541 - acc: 0.98 - ETA: 9s - loss: 0.0541 - acc: 0.9891 - ETA: 9s - loss: 0.0544 - acc: 0.989 - ETA: 9s - loss: 0.0551 - acc: 0.988 - ETA: 8s - loss: 0.0547 - acc: 0.989 - ETA: 8s - loss: 0.0544 - acc: 0.989 - ETA: 8s - loss: 0.0548 - acc: 0.989 - ETA: 8s - loss: 0.0543 - acc: 0.989 - ETA: 7s - loss: 0.0539 - acc: 0.989 - ETA: 7s - loss: 0.0539 - acc: 0.989 - ETA: 7s - loss: 0.0536 - acc: 0.989 - ETA: 6s - loss: 0.0532 - acc: 0.989 - ETA: 6s - loss: 0.0532 - acc: 0.989 - ETA: 6s - loss: 0.0531 - acc: 0.989 - ETA: 5s - loss: 0.0530 - acc: 0.989 - ETA: 5s - loss: 0.0533 - acc: 0.989 - ETA: 5s - loss: 0.0534 - acc: 0.989 - ETA: 5s - loss: 0.0530 - acc: 0.989 - ETA: 4s - loss: 0.0528 - acc: 0.989 - ETA: 4s - loss: 0.0524 - acc: 0.989 - ETA: 4s - loss: 0.0520 - acc: 0.989 - ETA: 3s - loss: 0.0517 - acc: 0.989 - ETA: 3s - loss: 0.0514 - acc: 0.989 - ETA: 3s - loss: 0.0513 - acc: 0.989 - ETA: 2s - loss: 0.0515 - acc: 0.989 - ETA: 2s - loss: 0.0516 - acc: 0.989 - ETA: 2s - loss: 0.0514 - acc: 0.989 - ETA: 2s - loss: 0.0513 - acc: 0.989 - ETA: 1s - loss: 0.0511 - acc: 0.989 - ETA: 1s - loss: 0.0508 - acc: 0.989 - ETA: 1s - loss: 0.0507 - acc: 0.989 - ETA: 0s - loss: 0.0507 - acc: 0.989 - ETA: 0s - loss: 0.0507 - acc: 0.989 - ETA: 0s - loss: 0.0505 - acc: 0.989 - ETA: 0s - loss: 0.0506 - acc: 0.989 - 23s 293us/step - loss: 0.0506 - acc: 0.9899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2058d015b00>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 10, batch_size = 1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成:9149/9150\r"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, w_to_ix, maxLen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-0429bcbe5d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtest_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "#去除前面augment效果\n",
    "test_sample = []\n",
    "for index, _ in enumerate(X_test):\n",
    "    test_sample.append([Y_test[index], X_test[index])\n",
    "test_sample = list(set(test_sample))\n",
    "len(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_oh = convert_to_one_hot(Y_test, C = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"../../data/X_test_indices.npy\",X_test_indices)\n",
    "# np.save(\"../../data/Y_test_oh.npy\",Y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9150/9150 [==============================] - 8s 912us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss,accuracy = model.evaluate(X_test_indices,Y_test_oh, batch_size = 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04932591845998999 0.9899453562986655\n"
     ]
    }
   ],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing 已完成:1/1\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00518612, 0.9948139 ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"Scammers are changing the contact details for banks on Google Maps to defraud people\"\n",
    "X =np.array([s])\n",
    "X = sentences_to_indices(X, w_to_ix, maxLen)\n",
    "model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
