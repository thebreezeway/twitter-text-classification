{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bc6f0404cfc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglorot_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean_url_replace_hashtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnlp_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_glove_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocess'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from math import floor\n",
    "\n",
    "from nlp_helper import read_glove_vecs, convert_to_one_hot, load_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef617b961e26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneg_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/twitter/6s8c/neg_14202_train.cleaned.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/twitter/6s8c/pos_14202_train.cleaned.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "neg_data, pos_data = load_corpus('../../data/twitter/6s8c/neg_14202_train.cleaned.txt'), load_corpus('../../data/twitter/6s8c/pos_14202_train.cleaned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_pos = np.array(pos_data).reshape(len(pos_data),)# shape of (m,)\n",
    "X_train_neg = np.array(neg_data).reshape(len(neg_data),)\n",
    "del neg_data, pos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_train_pos_indices_ori = np.load(\"../../data/twitter/X_train_pos_indices_14202.npy\")\n",
    "np.random.shuffle(X_train_pos_indices_ori)\n",
    "Y_train_pos_oh_ori =  convert_to_one_hot(np.array([ 1 for i in X_train_pos_indices_ori]),2)\n",
    "# X_train_pos_indices = sentences_to_indices(X_train_pos, w_to_ix, MAX_LEN)\n",
    "# np.save(\"../../data/X_train_pos_indices_14202.npy\",X_train_pos_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pos_indices, X_validate_pos_indices = X_train_pos_indices_ori[:1000], X_train_pos_indices_ori[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_pos = np.array([ 1 for i in X_train_pos_indices])\n",
    "Y_train_pos_oh  = convert_to_one_hot(Y_train_pos, C = 2)\n",
    "Y_validate_pos = np.array([ 1 for i in X_validate_pos_indices])\n",
    "Y_validate_pos_oh = convert_to_one_hot(Y_validate_pos, C = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_neg_indices = np.load(\"../../data/twitter/X_train_neg_indices_14202.npy\")\n",
    "# X_train_neg_indices = sentences_to_indices(X_train_neg, w_to_ix, MAX_LEN)\n",
    "# np.save(\"../../data/X_train_neg_indices_14202.npy\",X_train_neg_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_neg_indices, X_validate_neg_indices = X_train_neg_indices[:10000],X_train_neg_indices[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_neg = np.array([ 0 for i in X_train_neg_indices])\n",
    "Y_train_neg_oh= convert_to_one_hot(Y_train_neg, C = 2)\n",
    "Y_validate_neg = np.array([ 0 for i in X_validate_neg_indices])\n",
    "Y_validate_neg_oh = convert_to_one_hot(Y_validate_neg, C = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = np.concatenate((X_train_pos_indices, X_train_neg_indices), axis = 0)\n",
    "Y_train_oh = np.concatenate((Y_train_pos_oh, Y_train_neg_oh), axis = 0)\n",
    "X_validate_indices = np.concatenate((X_validate_pos_indices, X_validate_neg_indices), axis = 0)\n",
    "Y_validate_oh = np.concatenate((Y_validate_pos_oh, Y_validate_neg_oh), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "np.random.shuffle(X_train_indices)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(Y_train_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #加载训练数据\n",
    "# pos_datas = np.load(\"../../data/twitter/tech_not/pos_cleaned.npy\")\n",
    "# neg_datas = np.load(\"../../data/twitter/tech_not/neg_cleaned.npy\")\n",
    "\n",
    "# np.random.seed(1)\n",
    "# np.random.shuffle(pos_datas)\n",
    "# np.random.shuffle(neg_datas)\n",
    "\n",
    "# pos_datas_train, pos_datas_test = pos_datas[:700],pos_datas[700:800]\n",
    "# neg_datas_train_10, neg_datas_test_10 = neg_datas[:7000],neg_datas[7000:8000]\n",
    "# neg_datas_train_5, neg_datas_test_5 = neg_datas[13600:17100],neg_datas[17100:20000]\n",
    "# neg_datas_train_1_2 = neg_datas[20000:20350]\n",
    "# neg_datas_train_2, neg_datas_test_2 = neg_datas[12000:13400],neg_datas[13400:13600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_to_ix, ix_to_w, w_to_vec_map  = read_glove_vecs(\"../../data/embedding/glove.twitter.27B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train1_2 = np.concatenate((pos_datas_train,neg_datas_train_2), axis = 0)\n",
    "\n",
    "# np.random.shuffle(train1_2)\n",
    "# X_train_1_2 = train1_2[:,1]\n",
    "# Y_train_1_2 = np.array(list(map(int, train1_2[:,0])))\n",
    "# train1_5 = np.concatenate((pos_datas_train,neg_datas_train_5), axis = 0)\n",
    "\n",
    "# np.random.shuffle(train1_5)\n",
    "# X_train_1_5 = train1_5[:,1]\n",
    "# Y_train_1_5 = np.array(list(map(int, train1_5[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos_datas_50augs = np.tile(pos_datas, (50,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_auged = np.concatenate((pos_datas_50augs,neg_datas), axis = 0)\n",
    "# train_\n",
    "# np.random.shuffle(train_auged )\n",
    "\n",
    "# X_train, X_test = train_auged[:,1][0:80000], train_auged[:,1][80000:]\n",
    "# Y_train, Y_test = np.array(list(map(int, train_auged[:,0])))[0:80000], np.array(list(map(int, train_auged[:,0])))[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = nlp(X[i].lower())\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        \n",
    "        if len(sentence_words)> max_len:\n",
    "            continue\n",
    "        # Loop over the words of sentence_words\n",
    "        else:\n",
    "            j = 0\n",
    "            for w in sentence_words:\n",
    "\n",
    "                try:\n",
    "                    X_indices[i, j] = word_to_index[str(w)]\n",
    "                    j += 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "        print(\"已完成:{}/{}\".format(i+1,m),end='\\r')\n",
    "                \n",
    "\n",
    "    return X_indices\n",
    "\n",
    "def indices_to_sentence(X, index_to_word):\n",
    "    sentence = \"\"\n",
    "    for i in X :\n",
    "        if i!=0:\n",
    "            try:\n",
    "                sentence += (index_to_word[int(i)] + ' ')\n",
    "            except KeyError:\n",
    "                sentence += 'ERR '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_vec_map[\"cucumber\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        if word_to_vec_map[word].shape[0]!=50 :\n",
    "            print(\"word:\", word)\n",
    "            continue\n",
    "        emb_matrix[index] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Emojify_V2\n",
    "\n",
    "def twitter_tech_classify_V1(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype ='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(2)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation(activation='softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs =sentence_indices ,outputs=X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 80, 50)            59675700  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 80, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 80, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 59,899,190\n",
      "Trainable params: 223,490\n",
      "Non-trainable params: 59,675,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = twitter_tech_classify_V1((MAX_LEN,), w_to_vec_map, w_to_ix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights828.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11000 samples, validate on 3205 samples\n",
      "Epoch 1/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 6.6826 - acc: 0.976 - ETA: 0s - loss: 5.4455 - acc: 0.946 - 3s 237us/step - loss: 4.5737 - acc: 0.8944 - val_loss: 2.3631 - val_acc: 0.6343\n",
      "Epoch 2/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 2.3129 - acc: 0.674 - ETA: 0s - loss: 1.9661 - acc: 0.685 - 3s 236us/step - loss: 1.8434 - acc: 0.7059 - val_loss: 0.8455 - val_acc: 0.7738\n",
      "Epoch 3/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.6780 - acc: 0.824 - ETA: 0s - loss: 1.6550 - acc: 0.814 - 3s 235us/step - loss: 1.6011 - acc: 0.7915 - val_loss: 1.0518 - val_acc: 0.6802\n",
      "Epoch 4/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.4383 - acc: 0.636 - ETA: 0s - loss: 1.4790 - acc: 0.622 - 3s 238us/step - loss: 1.4970 - acc: 0.6168 - val_loss: 1.1885 - val_acc: 0.6324\n",
      "Epoch 5/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.5218 - acc: 0.652 - ETA: 0s - loss: 1.4636 - acc: 0.676 - 3s 234us/step - loss: 1.4428 - acc: 0.6916 - val_loss: 0.8341 - val_acc: 0.7560\n",
      "Epoch 6/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.3875 - acc: 0.788 - ETA: 0s - loss: 1.3533 - acc: 0.802 - 3s 235us/step - loss: 1.3729 - acc: 0.8014 - val_loss: 0.9586 - val_acc: 0.6615\n",
      "Epoch 7/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.3708 - acc: 0.691 - ETA: 0s - loss: 1.3550 - acc: 0.659 - 3s 236us/step - loss: 1.3215 - acc: 0.6735 - val_loss: 0.8323 - val_acc: 0.7417\n",
      "Epoch 8/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.1910 - acc: 0.781 - ETA: 0s - loss: 1.2236 - acc: 0.796 - 3s 233us/step - loss: 1.2272 - acc: 0.8014 - val_loss: 0.8016 - val_acc: 0.7457\n",
      "Epoch 9/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.2728 - acc: 0.795 - ETA: 0s - loss: 1.1582 - acc: 0.775 - 3s 233us/step - loss: 1.1556 - acc: 0.7664 - val_loss: 0.9170 - val_acc: 0.7092\n",
      "Epoch 10/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.1025 - acc: 0.758 - ETA: 0s - loss: 1.1032 - acc: 0.777 - 3s 235us/step - loss: 1.0945 - acc: 0.7885 - val_loss: 0.7643 - val_acc: 0.7616\n",
      "Epoch 11/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.9568 - acc: 0.823 - ETA: 0s - loss: 0.9753 - acc: 0.809 - 3s 235us/step - loss: 1.0472 - acc: 0.8020 - val_loss: 0.8371 - val_acc: 0.7454\n",
      "Epoch 12/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.0746 - acc: 0.806 - ETA: 0s - loss: 0.9619 - acc: 0.815 - 3s 236us/step - loss: 0.9789 - acc: 0.8215 - val_loss: 0.7217 - val_acc: 0.7875\n",
      "Epoch 13/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 1.0199 - acc: 0.841 - ETA: 0s - loss: 0.9631 - acc: 0.843 - 3s 234us/step - loss: 0.9586 - acc: 0.8415 - val_loss: 0.7986 - val_acc: 0.7666\n",
      "Epoch 14/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.8992 - acc: 0.827 - ETA: 0s - loss: 0.8993 - acc: 0.833 - 3s 236us/step - loss: 0.8928 - acc: 0.8410 - val_loss: 0.6527 - val_acc: 0.8097\n",
      "Epoch 15/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.8521 - acc: 0.872 - ETA: 0s - loss: 0.9276 - acc: 0.878 - 3s 237us/step - loss: 0.8794 - acc: 0.8752 - val_loss: 0.7876 - val_acc: 0.7760\n",
      "Epoch 16/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7960 - acc: 0.845 - ETA: 0s - loss: 0.9179 - acc: 0.840 - 3s 235us/step - loss: 0.8727 - acc: 0.8453 - val_loss: 0.6705 - val_acc: 0.8156\n",
      "Epoch 17/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7626 - acc: 0.885 - ETA: 0s - loss: 0.8220 - acc: 0.892 - 3s 238us/step - loss: 0.8098 - acc: 0.8951 - val_loss: 0.7033 - val_acc: 0.8131\n",
      "Epoch 18/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.8373 - acc: 0.890 - ETA: 0s - loss: 0.7719 - acc: 0.881 - 3s 234us/step - loss: 0.7853 - acc: 0.8805 - val_loss: 0.6770 - val_acc: 0.8315\n",
      "Epoch 19/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7424 - acc: 0.907 - ETA: 0s - loss: 0.7657 - acc: 0.912 - 3s 234us/step - loss: 0.7588 - acc: 0.9091 - val_loss: 0.7359 - val_acc: 0.8200\n",
      "Epoch 20/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7232 - acc: 0.893 - ETA: 0s - loss: 0.7807 - acc: 0.890 - 3s 235us/step - loss: 0.7526 - acc: 0.8926 - val_loss: 0.6168 - val_acc: 0.8406\n",
      "Epoch 21/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7056 - acc: 0.916 - ETA: 0s - loss: 0.6829 - acc: 0.921 - 3s 232us/step - loss: 0.7279 - acc: 0.9195 - val_loss: 0.6770 - val_acc: 0.8334\n",
      "Epoch 22/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6669 - acc: 0.912 - ETA: 0s - loss: 0.7310 - acc: 0.909 - 3s 234us/step - loss: 0.7277 - acc: 0.9065 - val_loss: 0.6545 - val_acc: 0.8424\n",
      "Epoch 23/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7273 - acc: 0.915 - ETA: 0s - loss: 0.7040 - acc: 0.924 - 3s 238us/step - loss: 0.6725 - acc: 0.9268 - val_loss: 0.6113 - val_acc: 0.8534\n",
      "Epoch 24/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7225 - acc: 0.927 - ETA: 0s - loss: 0.6541 - acc: 0.928 - 3s 233us/step - loss: 0.6519 - acc: 0.9277 - val_loss: 0.6534 - val_acc: 0.8474\n",
      "Epoch 25/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6434 - acc: 0.922 - ETA: 0s - loss: 0.6498 - acc: 0.926 - 3s 237us/step - loss: 0.6333 - acc: 0.9295 - val_loss: 0.5985 - val_acc: 0.8599\n",
      "Epoch 26/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6422 - acc: 0.939 - ETA: 0s - loss: 0.6535 - acc: 0.938 - 3s 234us/step - loss: 0.6330 - acc: 0.9384 - val_loss: 0.6887 - val_acc: 0.8431\n",
      "Epoch 27/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6685 - acc: 0.922 - ETA: 0s - loss: 0.6631 - acc: 0.922 - 3s 232us/step - loss: 0.6670 - acc: 0.9217 - val_loss: 0.6292 - val_acc: 0.8568\n",
      "Epoch 28/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7387 - acc: 0.927 - ETA: 0s - loss: 0.6513 - acc: 0.935 - 3s 236us/step - loss: 0.6320 - acc: 0.9388 - val_loss: 0.6280 - val_acc: 0.8543\n",
      "Epoch 29/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5807 - acc: 0.936 - ETA: 0s - loss: 0.6105 - acc: 0.931 - 3s 234us/step - loss: 0.6222 - acc: 0.9338 - val_loss: 0.6179 - val_acc: 0.8540\n",
      "Epoch 30/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6039 - acc: 0.937 - ETA: 0s - loss: 0.6449 - acc: 0.938 - 3s 245us/step - loss: 0.6171 - acc: 0.9394 - val_loss: 0.6420 - val_acc: 0.8518\n",
      "Epoch 31/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6288 - acc: 0.936 - ETA: 0s - loss: 0.6548 - acc: 0.936 - 3s 234us/step - loss: 0.6015 - acc: 0.9386 - val_loss: 0.6023 - val_acc: 0.8655\n",
      "Epoch 32/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4352 - acc: 0.948 - ETA: 0s - loss: 0.5248 - acc: 0.947 - 3s 233us/step - loss: 0.5866 - acc: 0.9462 - val_loss: 0.6116 - val_acc: 0.8643\n",
      "Epoch 33/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.7016 - acc: 0.943 - ETA: 0s - loss: 0.6062 - acc: 0.943 - 3s 236us/step - loss: 0.5814 - acc: 0.9451 - val_loss: 0.6099 - val_acc: 0.8665\n",
      "Epoch 34/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4541 - acc: 0.948 - ETA: 0s - loss: 0.4980 - acc: 0.948 - 3s 245us/step - loss: 0.5785 - acc: 0.9487 - val_loss: 0.5649 - val_acc: 0.8796\n",
      "Epoch 35/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6656 - acc: 0.950 - ETA: 0s - loss: 0.5954 - acc: 0.952 - 3s 244us/step - loss: 0.5856 - acc: 0.9506 - val_loss: 0.7089 - val_acc: 0.8484\n",
      "Epoch 36/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5971 - acc: 0.938 - ETA: 0s - loss: 0.5924 - acc: 0.932 - 3s 238us/step - loss: 0.6019 - acc: 0.9342 - val_loss: 0.6356 - val_acc: 0.8624\n",
      "Epoch 37/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6184 - acc: 0.944 - ETA: 0s - loss: 0.5933 - acc: 0.948 - 3s 241us/step - loss: 0.5999 - acc: 0.9504 - val_loss: 0.5821 - val_acc: 0.8721\n",
      "Epoch 38/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5377 - acc: 0.954 - ETA: 0s - loss: 0.5535 - acc: 0.947 - 3s 243us/step - loss: 0.5890 - acc: 0.9433 - val_loss: 0.6808 - val_acc: 0.8537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6380 - acc: 0.932 - ETA: 0s - loss: 0.5862 - acc: 0.933 - 3s 238us/step - loss: 0.5954 - acc: 0.9395 - val_loss: 0.5670 - val_acc: 0.8771\n",
      "Epoch 40/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6209 - acc: 0.953 - ETA: 0s - loss: 0.5685 - acc: 0.951 - 3s 244us/step - loss: 0.5768 - acc: 0.9504 - val_loss: 0.5949 - val_acc: 0.8705\n",
      "Epoch 41/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5653 - acc: 0.946 - ETA: 0s - loss: 0.5546 - acc: 0.947 - 3s 245us/step - loss: 0.5656 - acc: 0.9485 - val_loss: 0.5926 - val_acc: 0.8715\n",
      "Epoch 42/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6450 - acc: 0.952 - ETA: 0s - loss: 0.5994 - acc: 0.950 - 3s 248us/step - loss: 0.5646 - acc: 0.9515 - val_loss: 0.5846 - val_acc: 0.8733\n",
      "Epoch 43/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5572 - acc: 0.955 - ETA: 0s - loss: 0.5732 - acc: 0.952 - 3s 245us/step - loss: 0.5486 - acc: 0.9530 - val_loss: 0.5925 - val_acc: 0.8711\n",
      "Epoch 44/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6358 - acc: 0.950 - ETA: 0s - loss: 0.5824 - acc: 0.951 - 3s 246us/step - loss: 0.5439 - acc: 0.9537 - val_loss: 0.5831 - val_acc: 0.8736\n",
      "Epoch 45/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5566 - acc: 0.954 - ETA: 0s - loss: 0.5089 - acc: 0.955 - 3s 244us/step - loss: 0.5395 - acc: 0.9551 - val_loss: 0.5838 - val_acc: 0.8755\n",
      "Epoch 46/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5069 - acc: 0.960 - ETA: 0s - loss: 0.5493 - acc: 0.957 - 3s 242us/step - loss: 0.5311 - acc: 0.9573 - val_loss: 0.5529 - val_acc: 0.8793\n",
      "Epoch 47/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6352 - acc: 0.957 - ETA: 0s - loss: 0.5698 - acc: 0.956 - 3s 240us/step - loss: 0.5292 - acc: 0.9590 - val_loss: 0.5770 - val_acc: 0.8768\n",
      "Epoch 48/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4371 - acc: 0.962 - ETA: 0s - loss: 0.4692 - acc: 0.956 - 3s 244us/step - loss: 0.5279 - acc: 0.9567 - val_loss: 0.6265 - val_acc: 0.8708\n",
      "Epoch 49/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5484 - acc: 0.956 - ETA: 0s - loss: 0.5416 - acc: 0.955 - 3s 245us/step - loss: 0.5416 - acc: 0.9568 - val_loss: 0.5898 - val_acc: 0.8796\n",
      "Epoch 50/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4913 - acc: 0.957 - ETA: 0s - loss: 0.5461 - acc: 0.958 - 3s 241us/step - loss: 0.5229 - acc: 0.9592 - val_loss: 0.5831 - val_acc: 0.8811\n",
      "Epoch 51/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5947 - acc: 0.962 - ETA: 0s - loss: 0.5432 - acc: 0.958 - 3s 240us/step - loss: 0.5171 - acc: 0.9598 - val_loss: 0.5780 - val_acc: 0.8799\n",
      "Epoch 52/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5784 - acc: 0.962 - ETA: 0s - loss: 0.5347 - acc: 0.961 - 3s 244us/step - loss: 0.5294 - acc: 0.9602 - val_loss: 0.5677 - val_acc: 0.8842\n",
      "Epoch 53/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5912 - acc: 0.961 - ETA: 0s - loss: 0.5637 - acc: 0.962 - 3s 239us/step - loss: 0.5138 - acc: 0.9605 - val_loss: 0.5824 - val_acc: 0.8833\n",
      "Epoch 54/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4555 - acc: 0.960 - ETA: 0s - loss: 0.4429 - acc: 0.961 - 3s 242us/step - loss: 0.5199 - acc: 0.9603 - val_loss: 0.5858 - val_acc: 0.8817\n",
      "Epoch 55/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5357 - acc: 0.961 - ETA: 0s - loss: 0.4816 - acc: 0.961 - 3s 244us/step - loss: 0.5182 - acc: 0.9606 - val_loss: 0.5647 - val_acc: 0.8852\n",
      "Epoch 56/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5255 - acc: 0.962 - ETA: 0s - loss: 0.5086 - acc: 0.960 - 3s 245us/step - loss: 0.5195 - acc: 0.9615 - val_loss: 0.5572 - val_acc: 0.8877\n",
      "Epoch 57/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5643 - acc: 0.961 - ETA: 0s - loss: 0.4950 - acc: 0.962 - 3s 242us/step - loss: 0.4912 - acc: 0.9617 - val_loss: 0.5775 - val_acc: 0.8861\n",
      "Epoch 58/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4551 - acc: 0.961 - ETA: 0s - loss: 0.4596 - acc: 0.961 - 3s 241us/step - loss: 0.4973 - acc: 0.9613 - val_loss: 0.5890 - val_acc: 0.8836\n",
      "Epoch 59/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4934 - acc: 0.962 - ETA: 0s - loss: 0.4617 - acc: 0.962 - 3s 245us/step - loss: 0.4889 - acc: 0.9614 - val_loss: 0.5530 - val_acc: 0.8905\n",
      "Epoch 60/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4906 - acc: 0.960 - ETA: 0s - loss: 0.5230 - acc: 0.960 - 3s 245us/step - loss: 0.5038 - acc: 0.9620 - val_loss: 0.6247 - val_acc: 0.8777\n",
      "Epoch 61/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5295 - acc: 0.962 - ETA: 0s - loss: 0.5027 - acc: 0.960 - 3s 242us/step - loss: 0.4868 - acc: 0.9594 - val_loss: 0.6760 - val_acc: 0.8733\n",
      "Epoch 62/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4117 - acc: 0.961 - ETA: 0s - loss: 0.4593 - acc: 0.960 - 3s 243us/step - loss: 0.4984 - acc: 0.9598 - val_loss: 0.5879 - val_acc: 0.8852\n",
      "Epoch 63/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6633 - acc: 0.959 - ETA: 0s - loss: 0.5654 - acc: 0.962 - 3s 242us/step - loss: 0.5044 - acc: 0.9618 - val_loss: 0.6182 - val_acc: 0.8802\n",
      "Epoch 64/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4454 - acc: 0.959 - ETA: 0s - loss: 0.5236 - acc: 0.959 - 3s 238us/step - loss: 0.4827 - acc: 0.9590 - val_loss: 0.6938 - val_acc: 0.8727\n",
      "Epoch 65/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4125 - acc: 0.959 - ETA: 0s - loss: 0.4893 - acc: 0.957 - 3s 239us/step - loss: 0.4947 - acc: 0.9576 - val_loss: 0.6476 - val_acc: 0.8761\n",
      "Epoch 66/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4332 - acc: 0.943 - ETA: 0s - loss: 0.5805 - acc: 0.933 - 3s 240us/step - loss: 0.5993 - acc: 0.9287 - val_loss: 0.8335 - val_acc: 0.8368\n",
      "Epoch 67/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6128 - acc: 0.921 - ETA: 0s - loss: 0.6354 - acc: 0.920 - 3s 242us/step - loss: 0.6456 - acc: 0.9196 - val_loss: 0.8284 - val_acc: 0.8468\n",
      "Epoch 68/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5724 - acc: 0.931 - ETA: 0s - loss: 0.6413 - acc: 0.940 - 3s 243us/step - loss: 0.6542 - acc: 0.9418 - val_loss: 0.6455 - val_acc: 0.8711\n",
      "Epoch 69/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5153 - acc: 0.952 - ETA: 0s - loss: 0.5762 - acc: 0.949 - 3s 240us/step - loss: 0.5913 - acc: 0.9448 - val_loss: 0.8479 - val_acc: 0.8215\n",
      "Epoch 70/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6492 - acc: 0.919 - ETA: 0s - loss: 0.6779 - acc: 0.918 - 3s 243us/step - loss: 0.6607 - acc: 0.9224 - val_loss: 0.6636 - val_acc: 0.8602\n",
      "Epoch 71/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6211 - acc: 0.944 - ETA: 0s - loss: 0.6088 - acc: 0.946 - 3s 243us/step - loss: 0.6173 - acc: 0.9452 - val_loss: 0.7536 - val_acc: 0.8399\n",
      "Epoch 72/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6824 - acc: 0.941 - ETA: 0s - loss: 0.6115 - acc: 0.936 - 3s 246us/step - loss: 0.5953 - acc: 0.9353 - val_loss: 0.7029 - val_acc: 0.8477\n",
      "Epoch 73/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5789 - acc: 0.937 - ETA: 0s - loss: 0.5694 - acc: 0.942 - 3s 244us/step - loss: 0.6008 - acc: 0.9418 - val_loss: 0.6988 - val_acc: 0.8484\n",
      "Epoch 74/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6228 - acc: 0.936 - ETA: 0s - loss: 0.5316 - acc: 0.939 - 3s 243us/step - loss: 0.5473 - acc: 0.9383 - val_loss: 0.6624 - val_acc: 0.8555\n",
      "Epoch 75/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5494 - acc: 0.945 - ETA: 0s - loss: 0.5355 - acc: 0.947 - 3s 238us/step - loss: 0.5202 - acc: 0.9499 - val_loss: 0.5785 - val_acc: 0.8686\n",
      "Epoch 76/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.5387 - acc: 0.951 - ETA: 0s - loss: 0.5296 - acc: 0.955 - 3s 242us/step - loss: 0.5398 - acc: 0.9556 - val_loss: 0.6455 - val_acc: 0.8574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.6521 - acc: 0.946 - ETA: 0s - loss: 0.5775 - acc: 0.946 - 3s 244us/step - loss: 0.5160 - acc: 0.9482 - val_loss: 0.6575 - val_acc: 0.8593\n",
      "Epoch 78/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4489 - acc: 0.952 - ETA: 0s - loss: 0.4911 - acc: 0.952 - 3s 242us/step - loss: 0.4858 - acc: 0.9532 - val_loss: 0.6159 - val_acc: 0.8677\n",
      "Epoch 79/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4964 - acc: 0.957 - ETA: 0s - loss: 0.4622 - acc: 0.959 - 3s 240us/step - loss: 0.4757 - acc: 0.9588 - val_loss: 0.5614 - val_acc: 0.8786\n",
      "Epoch 80/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4920 - acc: 0.962 - ETA: 0s - loss: 0.4900 - acc: 0.961 - 3s 238us/step - loss: 0.4761 - acc: 0.9605 - val_loss: 0.6138 - val_acc: 0.8705\n",
      "Epoch 81/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4873 - acc: 0.955 - ETA: 0s - loss: 0.4379 - acc: 0.957 - 3s 240us/step - loss: 0.4479 - acc: 0.9578 - val_loss: 0.6563 - val_acc: 0.8721\n",
      "Epoch 82/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4199 - acc: 0.962 - ETA: 0s - loss: 0.4398 - acc: 0.959 - 3s 238us/step - loss: 0.4381 - acc: 0.9602 - val_loss: 0.6060 - val_acc: 0.8793\n",
      "Epoch 83/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4510 - acc: 0.961 - ETA: 0s - loss: 0.4021 - acc: 0.963 - 3s 242us/step - loss: 0.4233 - acc: 0.9625 - val_loss: 0.5529 - val_acc: 0.8833\n",
      "Epoch 84/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4043 - acc: 0.961 - ETA: 0s - loss: 0.4330 - acc: 0.962 - 3s 246us/step - loss: 0.4196 - acc: 0.9637 - val_loss: 0.5771 - val_acc: 0.8811\n",
      "Epoch 85/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4346 - acc: 0.961 - ETA: 0s - loss: 0.4151 - acc: 0.964 - 3s 242us/step - loss: 0.4041 - acc: 0.9637 - val_loss: 0.6219 - val_acc: 0.8799\n",
      "Epoch 86/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4382 - acc: 0.963 - ETA: 0s - loss: 0.4044 - acc: 0.963 - 3s 237us/step - loss: 0.3984 - acc: 0.9640 - val_loss: 0.6173 - val_acc: 0.8814\n",
      "Epoch 87/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3307 - acc: 0.964 - ETA: 0s - loss: 0.3595 - acc: 0.963 - 3s 247us/step - loss: 0.3859 - acc: 0.9643 - val_loss: 0.5976 - val_acc: 0.8827\n",
      "Epoch 88/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3414 - acc: 0.961 - ETA: 0s - loss: 0.3884 - acc: 0.962 - 3s 241us/step - loss: 0.3871 - acc: 0.9635 - val_loss: 0.6043 - val_acc: 0.8861\n",
      "Epoch 89/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3037 - acc: 0.964 - ETA: 0s - loss: 0.3444 - acc: 0.964 - 3s 241us/step - loss: 0.3650 - acc: 0.9642 - val_loss: 0.6736 - val_acc: 0.8780\n",
      "Epoch 90/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.4732 - acc: 0.962 - ETA: 0s - loss: 0.3821 - acc: 0.962 - 3s 242us/step - loss: 0.3625 - acc: 0.9605 - val_loss: 0.6894 - val_acc: 0.8764\n",
      "Epoch 91/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3251 - acc: 0.959 - ETA: 0s - loss: 0.3344 - acc: 0.962 - 3s 243us/step - loss: 0.3292 - acc: 0.9634 - val_loss: 0.6381 - val_acc: 0.8833\n",
      "Epoch 92/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3392 - acc: 0.962 - ETA: 0s - loss: 0.3137 - acc: 0.963 - 3s 241us/step - loss: 0.3110 - acc: 0.9639 - val_loss: 0.5954 - val_acc: 0.8889\n",
      "Epoch 93/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3013 - acc: 0.965 - ETA: 0s - loss: 0.2796 - acc: 0.964 - 3s 241us/step - loss: 0.3240 - acc: 0.9638 - val_loss: 0.6270 - val_acc: 0.8858\n",
      "Epoch 94/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3411 - acc: 0.959 - ETA: 0s - loss: 0.3081 - acc: 0.957 - 3s 240us/step - loss: 0.3090 - acc: 0.9556 - val_loss: 0.6684 - val_acc: 0.8739\n",
      "Epoch 95/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.2759 - acc: 0.951 - ETA: 0s - loss: 0.2985 - acc: 0.954 - 3s 244us/step - loss: 0.3035 - acc: 0.9552 - val_loss: 0.6504 - val_acc: 0.8802\n",
      "Epoch 96/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.2791 - acc: 0.953 - ETA: 0s - loss: 0.3275 - acc: 0.957 - 3s 242us/step - loss: 0.3074 - acc: 0.9588 - val_loss: 0.6375 - val_acc: 0.8761\n",
      "Epoch 97/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.3041 - acc: 0.958 - ETA: 0s - loss: 0.2774 - acc: 0.958 - 3s 243us/step - loss: 0.2880 - acc: 0.9595 - val_loss: 0.6328 - val_acc: 0.8805\n",
      "Epoch 98/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.2546 - acc: 0.956 - ETA: 0s - loss: 0.2626 - acc: 0.963 - 3s 243us/step - loss: 0.2623 - acc: 0.9647 - val_loss: 0.6371 - val_acc: 0.8867\n",
      "Epoch 99/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.2801 - acc: 0.962 - ETA: 0s - loss: 0.2646 - acc: 0.965 - 3s 241us/step - loss: 0.2772 - acc: 0.9657 - val_loss: 0.6941 - val_acc: 0.8789\n",
      "Epoch 100/100\n",
      "11000/11000 [==============================] - ETA: 1s - loss: 0.2195 - acc: 0.968 - ETA: 0s - loss: 0.2355 - acc: 0.966 - 3s 241us/step - loss: 0.2701 - acc: 0.9668 - val_loss: 0.6801 - val_acc: 0.8780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cfaf988e48>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, validation_data=[X_validate_indices, Y_validate_oh], epochs = 100, batch_size = 4096, shuffle=True, class_weight = {0:1,1:50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344/1344 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy = model.evaluate(X_train_pos_indices_ori,Y_train_pos_oh_ori, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.183562170271 0.94568452381\n"
     ]
    }
   ],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成:1/1\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[211511.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = np.array([\"food\"])\n",
    "# X = sentences_to_indices(X, w_to_ix, MAX_LEN)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = model.predict(X_validate_pos_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_validate_pos_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "thanks to the virtual reality lab for the incredible training on mass handling in space ! and i practiced handling several pieces of equipment , to include a pound ammonia tank nbd . … \n",
      "\n",
      "\n",
      "1\n",
      "delivering battle winning air power : typhoon jets from continue to degrade and on operations in the middleeast as part of the . . \n",
      "\n",
      "\n",
      "2\n",
      ". ’s deep space exploration program was highlighted at madeinamerica hosted by the . we ’re a proud contributor to , & . learn more about : \n",
      "\n",
      "\n",
      "3\n",
      "the nightshift is about to get a new friend . has partnered with to bring the world ’s first autonomous indoor surveillance drone . \n",
      "\n",
      "\n",
      "4\n",
      "look at those baby planets ! an international team of astronomers used archival radio telescope data & supercomputers to develop a new method for finding very young planets : . \n",
      "\n",
      "\n",
      "5\n",
      "blockchain data now available on google cloud ! a live updating public dataset for smart contract analytics by and cc blockchain \n",
      "\n",
      "\n",
      "6\n",
      "with suppliers in all states from hawaii to maine , the deep space exploration system is being built to carry us to the moon and beyond . madeinamerica \n",
      "\n",
      "\n",
      "7\n",
      "cloud adoption in large companies was a key discussion point at ltw - hear from john abel and mark cotton : \n",
      "\n",
      "\n",
      "8\n",
      "[ ] is ready to welcome you at booth ! get the chance to meet with and giulia to discover ® technology , the sole platform to produce challenging , monolithic shapes in glass with high precision ! \n",
      "\n",
      "\n",
      "9\n",
      "joint statement of & , signed by agency directors & avi , reaffirming the strong mutual interest in strengthening cooperation between the us and israel in space exploration , research and space … \n",
      "\n",
      "\n",
      "10\n",
      "while is busy in mars orbit , our rover teams are busy on earth , prepping components of the next mission in one of the cleanest ! … \n",
      "\n",
      "\n",
      "11\n",
      "imaging will be at farnborough international airshow - hall , space zone , uk space agency / esa pavilion - stand - to july . \n",
      "\n",
      "\n",
      "12\n",
      "mobile networks continue to have an important play in automotive . & have developed an alliance to bring cars “ online ” for more intelligent connected vehicles . shares more : \n",
      "\n",
      "\n",
      "13\n",
      "u - battery general manager , steve , said : “ u - battery is delighted to have received the green light to progress to phase of the uk government ’s amr programme . \" \n",
      "\n",
      "\n",
      "14\n",
      "presentation of results on generation for g using microwave photonics at \n",
      "\n",
      "\n",
      "15\n",
      "first laser light for grace follow - on . near earth network nen provides comm and tracking services to grace - fo . … \n",
      "\n",
      "\n",
      "16\n",
      "biotech company invests in uk - based company developing real - time genetic sequencing technology … \n",
      "\n",
      "\n",
      "17\n",
      "currently in orbit around the moon , has taken our view of the moon into the century . with years in orbit and counting , lro has demonstrated that the moon is more dynamic than we previously thought . \n",
      "\n",
      "\n",
      "18\n",
      "take guesswork out of your technology investments , the stakes are too high . our latest research provides a road map to select and adopt the right technologies for your organization . … \n",
      "\n",
      "\n",
      "19\n",
      "perception drives reality . vice chairman at morgan stanley , shares her “ hard - earned , hard - learned pearls of wisdom ” that can help you shape the way others perceive you : \n",
      "\n",
      "\n",
      "20\n",
      "australian synchrotron leader honoured - \n",
      "\n",
      "\n",
      "21\n",
      "on its way to the sun , parker solar probe will reach speeds up to miles per hour — fast enough to get from philadelphia to washington in a second — setting the record for the fastest spacecraft in history . stay tuned — parker is about to take flight . \n",
      "\n",
      "\n",
      "22\n",
      ". sharing his thoughts on the race to g and what could result from innovative tech created in the u.s . … \n",
      "\n",
      "\n",
      "23\n",
      "pushed , pulled , bent . the test version of the is undergoing millions of pounds of force at to ensure it can withstand the forces of launch and ascent . learn more > > \n",
      "\n",
      "\n",
      "24\n",
      "the team fired up second stage this week for our upcoming mission , our next launch after . we wait to fly the exciting on this mission . \n",
      "\n",
      "\n",
      "25\n",
      "i bet the dinosaurs wish they had spaceships back in their day ! is doing important work for human space exploration to make sure that we do not end up as a single planet species . visit to find out \n",
      "\n",
      "\n",
      "26\n",
      "maintaining a stable grid with a mix of energy sources is no easy task . that ’s why we built an energy reservoir – to help keep the grid running smoothly . \n",
      "\n",
      "\n",
      "27\n",
      "with smart , diabetes patients wo n’t need to spend time thinking about how to control blood sugar . \" the drug does the thinking for them , \" says scientist jonathan rosen \n",
      "\n",
      "\n",
      "28\n",
      "falcon and merah putih are vertical on pad in florida . weather is % favorable for the two - hour launch window , which opens tuesday , august at a.m. est , a.m. utc . \n",
      "\n",
      "\n",
      "29\n",
      "\" machine learning techniques for space weather \" , eds . e. , j. johnson , & s. wing ( elsevier ) … \n",
      "\n",
      "\n",
      "30\n",
      "bernard says the future of chemistry is molecular machinery inspired by nature : pharmaceuticals , self - healing materials , . all in development now , because the best way to predict the future is to invent it . \n",
      "\n",
      "\n",
      "31\n",
      "technology used to help produce glass molds for droplet … \n",
      "\n",
      "\n",
      "32\n",
      ". is developing a “ face matching ” feature that could be integrated with google assistant . similar to ’s feature , trusted face , the new feature will collect data via images & videos \n",
      "\n",
      "\n",
      "33\n",
      "did someone say quantum ? of excitement about hearing on the department of efforts in the field of quantum information live here → \n",
      "\n",
      "\n",
      "34\n",
      "our drone looked different because we built a instead of a flat plate . - william , ‘ bs aero / astro ; mark van bergen , ‘ ms , simon li , ‘ ms , shen , ‘ ms , aero / astro ; garth edwards ms , graduate school of business \n",
      "\n",
      "\n",
      "35\n",
      "excited to hear about business opportunities in eastern cleantech sector ! germany business energy water – 在 world trade center \n",
      "\n",
      "\n",
      "36\n",
      ". postdoc and recent grad seth used hpc simulation to discover surprising behavior when plasmas are compressed : it becomes more turbulent until it dissipates as heat . he gives the details as he picks up his howes scholar award at . \n",
      "\n",
      "\n",
      "37\n",
      "today , baidu and group jointly launched a smart mini - program which allows visitors to its safari park to learn about and “ interact ” with wild animals through ar functions of the flagship baidu app . check out this video we took at the park to learn more ! \n",
      "\n",
      "\n",
      "38\n",
      "what would it be like to follow in the footsteps of neil armstrong and the other astronauts ? now you can find out , in a vr experience created using details , data and images from read more : … \n",
      "\n",
      "\n",
      "39\n",
      "this winner uses a pre - trained model with attention to classify from chest x - rays : \n",
      "\n",
      "\n",
      "40\n",
      "\" tech takes lead with groundbreaking \" - astronomy science … \n",
      "\n",
      "\n",
      "41\n",
      "new data reveals how some of russia ’s online trolls went viral … via \n",
      "\n",
      "\n",
      "42\n",
      "been a week since successful launch of new galileo satellites . we are now covering the whole world on a permanent basis . galileo is the most precise navigation system in the world . thanks to all the great people that made it happen ! \n",
      "\n",
      "\n",
      "43\n",
      "inflatable decoy tanks to puff up army ’s strength given the current state of the mod budget , this is too close to parody for comfort … \n",
      "\n",
      "\n",
      "44\n",
      "check out eyes on the dsn to see real time status of communications with our deep space explorers , such as … \n",
      "\n",
      "\n",
      "45\n",
      "ceo robin li announced the first minibus with are rolling off the plant \n",
      "\n",
      "\n",
      "46\n",
      "in collaboration with the marco initiative ( ) , we customized deep networks to achieve an accuracy of % on the visual recognition task of identifying protein crystals , a key step in discovering new targeted \n",
      "\n",
      "\n",
      "47\n",
      "a method to add layers into hierarchical all - dna materials . \n",
      "\n",
      "\n",
      "48\n",
      "thanks for the ride to get to orbit ! wishing booster a successful second flight to . … \n",
      "\n",
      "\n",
      "49\n",
      "corning gorilla glass for automotive has been awarded more than platforms to date . \n",
      "\n",
      "\n",
      "50\n",
      "a new study shows that climate change could cause pest - related crop losses to what that means for the northwest and largest ( by weight ) export : wheat . … \n",
      "\n",
      "\n",
      "51\n",
      "we are going back to the moon so that we can prove the technologies we need to land americans on \n",
      "\n",
      "\n",
      "52\n",
      "tune in to watch live as these university students present their solutions to human spaceflight challenges ! … \n",
      "\n",
      "\n",
      "53\n",
      ": rt : interplanetary internet could soon be a reality , due to ! delay / disruption tolerant networking puts … \n",
      "\n",
      "\n",
      "54\n",
      "the eight were built to study tropical cyclones . a new and unexpected capability has emerged : the ability to see through clouds and rain to flooded landscapes . \n",
      "\n",
      "\n",
      "55\n",
      ". : \" mm wave spectrum can give us a boost in achieving g \" \n",
      "\n",
      "\n",
      "56\n",
      "semiconductors forms subsidiary micro to make micro uv - led chips for displays \n",
      "\n",
      "\n",
      "57\n",
      "we “ are the world ’s leading robot builder and it came from space . ” great video on where we ’ve been as a country exploring off the planet . i wonder where we ’ll end up going ? … \n",
      "\n",
      "\n",
      "58\n",
      "on the day that airbus and bmw got real on , this is the news lead in the guardian beyond … \n",
      "\n",
      "\n",
      "59\n",
      "we have new papers nanoparticles enable low threshold cw lasing . with a by gungun lin & jin \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i,a in enumerate(ans):\n",
    "    if a[0]>a[1]:\n",
    "        \n",
    "        print(count)\n",
    "        print(indices_to_sentence(X_validate_pos_indices[i],ix_to_w))\n",
    "        print('\\n')\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"weights100epoch_978_878_828.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_file(path):\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            p.set\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r', encoding = 'utf8') as f:\n",
    "        buffer = f.readlines()\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
